import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from io import BytesIO
import json
from datetime import datetime
import cv2
import numpy as np
from PIL import Image
from sklearn.cluster import KMeans
import re

# Configure page
st.set_page_config(
    page_title="OMR Evaluation System",
    page_icon="üìù",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #2E86AB;
        margin-bottom: 2rem;
    }
    .metric-container {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .score-high {
        background-color: #d4edda;
        color: #155724;
        padding: 0.5rem;
        border-radius: 0.25rem;
        font-weight: bold;
    }
    .score-medium {
        background-color: #fff3cd;
        color: #856404;
        padding: 0.5rem;
        border-radius: 0.25rem;
        font-weight: bold;
    }
    .score-low {
        background-color: #f8d7da;
        color: #721c24;
        padding: 0.5rem;
        border-radius: 0.25rem;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

class AnswerKeyLoader:
    """Load and parse answer keys from Excel files"""

    def __init__(self):
        self.answer_keys = {}

    def load_from_excel(self, excel_file):
        """Load answer keys from uploaded Excel file"""
        try:
            # Try to load from the new format first (single sheet with both sets)
            df = pd.read_excel(excel_file)

            # Check if this is the new format (columns like 'Python', 'EDA', etc.)
            if self._is_new_format(df):
                # Parse the new format where both sets are in one sheet
                self.answer_keys['A'] = self._parse_new_format(df, 'A')
                self.answer_keys['B'] = self._parse_new_format(df, 'B')
                return len(self.answer_keys) > 0
            else:
                # Fall back to old format with separate sheets
                xl_file = pd.ExcelFile(excel_file)

                for sheet_name in xl_file.sheet_names:
                    if 'A' in sheet_name.upper():
                        df_a = pd.read_excel(excel_file, sheet_name=sheet_name)
                        self.answer_keys['A'] = self._parse_answer_sheet(df_a, 'Set A')
                    elif 'B' in sheet_name.upper():
                        df_b = pd.read_excel(excel_file, sheet_name=sheet_name)
                        self.answer_keys['B'] = self._parse_answer_sheet(df_b, 'Set B')

                return len(self.answer_keys) > 0
        except Exception as e:
            st.error(f"Error loading answer keys: {str(e)}")
            return False

    def _parse_answer_sheet(self, df, set_name):
        """Parse answer sheet DataFrame"""
        answers = {}
        subjects = ['PYTHON', 'EDA', 'SQL', 'POWER BI', 'ADV STATS']

        # Clean column names
        columns = [col.strip() if isinstance(col, str) else col for col in df.columns]
        df.columns = columns

        try:
            for subject_idx, subject in enumerate(subjects):
                # Find answer column for this subject
                answer_col = None
                for col in columns:
                    if isinstance(col, str) and (subject.lower() in col.lower() or
                                               'answer' in col.lower()):
                        answer_col = col
                        break

                if answer_col and answer_col in df.columns:
                    # Extract answers for this subject (20 questions per subject)
                    start_q = subject_idx * 20 + 1
                    end_q = (subject_idx + 1) * 20

                    subject_answers = df[answer_col].dropna().head(20)

                    for i, answer in enumerate(subject_answers):
                        q_num = start_q + i
                        if isinstance(answer, str):
                            answer = answer.upper().strip()
                            if answer in ['A', 'B', 'C', 'D']:
                                answers[q_num] = {
                                    'answer': answer,
                                    'subject': subject,
                                    'marks': 1
                                }

        except Exception as e:
            st.error(f"Error parsing {set_name}: {str(e)}")

        return answers

    def _is_new_format(self, df):
        """Check if this is the new Excel format with subject columns"""
        expected_subjects = ['PYTHON', 'EDA', 'SQL', 'POWER BI', 'STATISTICS']
        columns = [str(col).strip().upper() for col in df.columns]

        # Check if at least 3 expected subjects are present in columns
        matches = sum(1 for subject in expected_subjects if any(subject in col for col in columns))
        return matches >= 3

    def _parse_new_format(self, df, set_letter):
        """Parse the new Excel format with 'question_number - answer_option' format"""
        answers = {}

        # Map Excel columns to our subject names
        subject_mapping = {
            'PYTHON': 'PYTHON',
            'EDA': 'EDA',
            'SQL': 'SQL',
            'POWER BI': 'POWER BI',
            'STATISTICS': 'STATISTICS',
            'SATISTICS': 'STATISTICS'  # Handle typo in Excel
        }

        # Clean column names
        columns = [str(col).strip() for col in df.columns]

        try:
            for col_name in columns:
                col_upper = col_name.upper()

                # Find which subject this column represents
                subject_key = None
                for excel_subject, our_subject in subject_mapping.items():
                    if excel_subject in col_upper:
                        subject_key = our_subject
                        break

                if not subject_key:
                    continue

                # Find the original column name (may have whitespace)
                original_col_name = None
                for orig_col in df.columns:
                    if str(orig_col).strip().upper() == col_upper:
                        original_col_name = orig_col
                        break

                if not original_col_name:
                    continue

                # Get non-empty values from this column
                col_data = df[original_col_name].dropna().head(20)  # Max 20 questions per subject

                for cell_value in col_data:
                    if pd.isna(cell_value):
                        continue

                    cell_str = str(cell_value).strip()
                    if not cell_str:
                        continue

                    # Parse "question_number - answer_option" or "question_number. answer_option"
                    parsed_data = self._parse_question_answer(cell_str)
                    if parsed_data:
                        q_num, answer_options = parsed_data

                        # For now, we'll use the first answer option for both sets
                        # This assumes the Excel contains answers for Set A
                        # You may need to modify this logic based on your specific requirements
                        if set_letter == 'A':
                            primary_answer = answer_options[0] if answer_options else None
                        else:  # Set B - could use same answers or different logic
                            primary_answer = answer_options[0] if answer_options else None

                        if primary_answer:
                            answers[q_num] = {
                                'answer': primary_answer.upper(),
                                'subject': subject_key,
                                'marks': 1,
                                'all_options': answer_options  # Store all options for multi-answer questions
                            }

        except Exception as e:
            st.error(f"Error parsing new format for Set {set_letter}: {str(e)}")

        return answers

    def _parse_question_answer(self, cell_value):
        """Parse 'question_number - answer_option' format"""
        try:
            # Handle formats like "1 - a", "21 - a", "81. a", etc.
            # Split by '-' or '.'
            if ' - ' in cell_value:
                parts = cell_value.split(' - ')
            elif '. ' in cell_value:
                parts = cell_value.split('. ')
            else:
                return None

            if len(parts) != 2:
                return None

            # Extract question number
            q_num_str = parts[0].strip()
            if not q_num_str.isdigit():
                return None
            q_num = int(q_num_str)

            # Extract answer options
            answer_part = parts[1].strip().lower()

            # Handle multiple answers like "a,b,c,d" or single answers like "a"
            if ',' in answer_part:
                answer_options = [opt.strip() for opt in answer_part.split(',')]
            else:
                answer_options = [answer_part]

            # Validate answer options
            valid_answers = ['a', 'b', 'c', 'd']
            answer_options = [opt for opt in answer_options if opt in valid_answers]

            if not answer_options:
                return None

            return q_num, answer_options

        except Exception:
            return None

    def get_answer_key(self, set_letter):
        """Get answer key for specific set"""
        return self.answer_keys.get(set_letter, {})

class ScoringEngine:
    """OMR Scoring Engine"""

    def __init__(self, answer_key_loader):
        self.answer_key_loader = answer_key_loader
        self.subjects = ['PYTHON', 'EDA', 'SQL', 'POWER BI', 'STATISTICS']

    def score_omr_sheet(self, extracted_answers, set_letter, student_info=None):
        """Score an OMR sheet against answer key"""
        answer_key = self.answer_key_loader.get_answer_key(set_letter)

        if not answer_key:
            return {'error': f'No answer key found for Set {set_letter}'}

        results = {
            'student_info': student_info or {},
            'set': set_letter,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'subject_scores': {},
            'total_score': 0,
            'max_possible_score': 100,
            'percentage': 0.0,
            'question_details': {},
            'summary': {
                'correct': 0,
                'incorrect': 0,
                'unanswered': 0,
                'total_questions': len(answer_key)
            }
        }

        # Initialize subject scores
        for subject in self.subjects:
            results['subject_scores'][subject] = {
                'correct': 0,
                'total': 20,
                'score': 0,
                'percentage': 0.0
            }

        # Score each question
        for q_num, correct_data in answer_key.items():
            correct_answer = correct_data['answer']
            subject = correct_data['subject']
            marks = correct_data.get('marks', 1)

            # Get student's answer
            student_data = extracted_answers.get(str(q_num), {})
            student_answer = student_data.get('answer', '').upper() if student_data.get('answer') else None
            confidence = student_data.get('confidence', 'none')

            # Determine correctness
            if student_answer is None:
                status = 'unanswered'
                score = 0
                results['summary']['unanswered'] += 1
            elif student_answer == correct_answer:
                status = 'correct'
                score = marks
                results['summary']['correct'] += 1
                results['subject_scores'][subject]['correct'] += 1
            else:
                status = 'incorrect'
                score = 0
                results['summary']['incorrect'] += 1

            # Store question details
            results['question_details'][q_num] = {
                'subject': subject,
                'correct_answer': correct_answer,
                'student_answer': student_answer,
                'status': status,
                'score': score,
                'confidence': confidence,
                'marks': marks
            }

            results['total_score'] += score

        # Calculate percentages
        results['percentage'] = (results['total_score'] / results['max_possible_score']) * 100

        for subject in results['subject_scores']:
            subject_score = results['subject_scores'][subject]
            subject_score['score'] = subject_score['correct'] * 1  # 1 mark per question
            subject_score['percentage'] = (subject_score['correct'] / subject_score['total']) * 100

        return results

class OMRProcessor:
    """Complete OMR processor for Streamlit Cloud"""

    def __init__(self):
        self.subjects = ['PYTHON', 'EDA', 'SQL', 'POWER BI', 'ADV STATS']
        self.expected_columns = 5
        self.expected_rows_per_column = 20
        self.bubbles_per_question = 4

    def detect_circles(self, image):
        """Detect circles using HoughCircles with enhanced parameter sets"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Enhanced preprocessing
        blurred = cv2.GaussianBlur(gray, (7, 7), 1.5)

        # Apply histogram equalization for better contrast
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        enhanced = clahe.apply(blurred)

        # Enhanced parameter sets for better detection
        param_sets = [
            {'dp': 1.0, 'min_dist': 20, 'param1': 40, 'param2': 20, 'min_radius': 8, 'max_radius': 30},
            {'dp': 1.2, 'min_dist': 25, 'param1': 50, 'param2': 25, 'min_radius': 10, 'max_radius': 28},
            {'dp': 1.5, 'min_dist': 22, 'param1': 60, 'param2': 30, 'min_radius': 12, 'max_radius': 32},
            {'dp': 0.8, 'min_dist': 18, 'param1': 35, 'param2': 18, 'min_radius': 6, 'max_radius': 35}
        ]

        all_circles = []
        for params in param_sets:
            circles = cv2.HoughCircles(
                enhanced, cv2.HOUGH_GRADIENT,
                dp=params['dp'],
                minDist=params['min_dist'],
                param1=params['param1'],
                param2=params['param2'],
                minRadius=params['min_radius'],
                maxRadius=params['max_radius']
            )

            if circles is not None:
                circles = np.round(circles[0, :]).astype("int")
                all_circles.extend(circles)

        # Remove duplicates with smaller threshold
        if all_circles:
            all_circles = self.remove_duplicate_circles(all_circles, min_distance=12)

        return all_circles

    def remove_duplicate_circles(self, circles, min_distance=12):
        """Remove duplicate circles that are too close to each other"""
        if len(circles) <= 1:
            return circles

        unique_circles = []
        for circle in circles:
            x, y, r = circle
            is_duplicate = False

            for unique_circle in unique_circles:
                ux, uy, ur = unique_circle
                distance = np.sqrt((x - ux)**2 + (y - uy)**2)

                if distance < min_distance:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_circles.append(circle)

        return unique_circles

    def organize_bubbles_into_grid(self, circles, image_shape):
        """Organize detected circles into a column-based grid structure"""
        if len(circles) < 50:
            return {}

        # Extract coordinates
        positions = np.array([[x, y] for x, y, r in circles])

        # Cluster by columns (X-coordinates)
        try:
            n_clusters = min(self.expected_columns, len(positions))
            kmeans_cols = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            col_labels = kmeans_cols.fit_predict(positions[:, 0].reshape(-1, 1))
        except:
            return {}

        organized_grid = {}

        for col_idx in range(n_clusters):
            # Get circles for this column
            col_mask = col_labels == col_idx
            col_circles = [circles[i] for i in range(len(circles)) if col_mask[i]]

            if len(col_circles) < 20:  # Need minimum circles per column
                continue

            # Sort by Y-coordinate (top to bottom)
            col_circles.sort(key=lambda c: c[1])

            # Group into rows (questions) - every 4 bubbles is one question
            for row_start in range(0, len(col_circles) - 3, 4):
                question_bubbles = col_circles[row_start:row_start + 4]

                if len(question_bubbles) == 4:
                    # Calculate question number (column-based numbering)
                    question_num = col_idx * self.expected_rows_per_column + (row_start // 4) + 1

                    if question_num <= 100:
                        # Sort bubbles left to right (A, B, C, D)
                        question_bubbles.sort(key=lambda c: c[0])

                        organized_grid[question_num] = {
                            'subject': self.subjects[col_idx] if col_idx < len(self.subjects) else 'Unknown',
                            'bubbles': question_bubbles,
                            'column': col_idx,
                            'row': row_start // 4
                        }

        return organized_grid

    def classify_bubbles(self, image, bubbles):
        """Enhanced bubble classification using multiple detection methods"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        classifications = []

        # Global image statistics
        global_mean = np.mean(gray)
        global_std = np.std(gray)

        # Pre-analyze all bubbles for this specific set
        bubble_stats = []
        for x, y, r in bubbles:
            mask = np.zeros(gray.shape, dtype=np.uint8)
            cv2.circle(mask, (x, y), max(1, r - 1), 255, -1)
            bubble_pixels = gray[mask == 255]

            if len(bubble_pixels) > 0:
                mean_intensity = np.mean(bubble_pixels)
                min_intensity = np.min(bubble_pixels)
                std_intensity = np.std(bubble_pixels)

                # Surrounding area analysis
                surrounding_mask = np.zeros(gray.shape, dtype=np.uint8)
                cv2.circle(surrounding_mask, (x, y), r + 4, 255, -1)
                cv2.circle(surrounding_mask, (x, y), r - 1, 0, -1)
                surrounding_pixels = gray[surrounding_mask == 255]
                surrounding_mean = np.mean(surrounding_pixels) if len(surrounding_pixels) > 0 else global_mean

                bubble_stats.append({
                    'mean': mean_intensity,
                    'min': min_intensity,
                    'std': std_intensity,
                    'surrounding': surrounding_mean,
                    'ratio': mean_intensity / surrounding_mean if surrounding_mean > 0 else 1.0
                })
            else:
                bubble_stats.append({'mean': 255, 'min': 255, 'std': 0, 'surrounding': 255, 'ratio': 1.0})

        # Calculate dynamic thresholds for this specific question's bubbles
        if len(bubble_stats) == 4:  # Exactly 4 bubbles for this question
            intensities = [s['mean'] for s in bubble_stats]
            ratios = [s['ratio'] for s in bubble_stats]

            # Sort to identify potential filled bubbles
            intensity_sorted = sorted(intensities)
            ratio_sorted = sorted(ratios)

            # Dynamic thresholds based on the relative darkness within this question
            # If there's a significant gap between darkest and others, use that
            intensity_gap = intensity_sorted[1] - intensity_sorted[0] if len(intensity_sorted) > 1 else 0
            ratio_gap = ratio_sorted[1] - ratio_sorted[0] if len(ratio_sorted) > 1 else 0

            # Thresholds for this specific question
            if intensity_gap > 30:  # Clear dark bubble present
                intensity_threshold = (intensity_sorted[0] + intensity_sorted[1]) / 2
            else:
                intensity_threshold = intensity_sorted[0] + 20  # More lenient

            if ratio_gap > 0.2:  # Clear ratio difference
                ratio_threshold = (ratio_sorted[0] + ratio_sorted[1]) / 2
            else:
                ratio_threshold = ratio_sorted[0] + 0.15  # More lenient

        else:
            # Fallback for non-standard bubble counts
            intensity_threshold = global_mean * 0.7
            ratio_threshold = 0.8

        # Classify each bubble
        for i, (x, y, r) in enumerate(bubbles):
            if i < len(bubble_stats):
                stats = bubble_stats[i]

                # Multiple classification criteria
                criteria_met = 0

                # Criterion 1: Absolute darkness
                if stats['mean'] < intensity_threshold:
                    criteria_met += 2  # Strong indicator

                # Criterion 2: Ratio to surrounding
                if stats['ratio'] < ratio_threshold:
                    criteria_met += 2  # Strong indicator

                # Criterion 3: Minimum intensity (very dark pixels present)
                if stats['min'] < (intensity_threshold * 0.8):
                    criteria_met += 1

                # Criterion 4: Low standard deviation (consistent darkness)
                if stats['std'] < 30:
                    criteria_met += 1

                # Criterion 5: Much darker than global mean
                if stats['mean'] < (global_mean * 0.6):
                    criteria_met += 1

                # Criterion 6: Darkest in this set of 4 bubbles (relative comparison)
                if len(bubble_stats) == 4:
                    darkest_index = min(range(4), key=lambda x: bubble_stats[x]['mean'])
                    if i == darkest_index and bubble_stats[i]['mean'] < (global_mean * 0.85):
                        criteria_met += 1

                # Final classification: need at least 3 criteria or 2 strong criteria
                is_filled = criteria_met >= 3

                classifications.append(is_filled)
            else:
                classifications.append(False)

        return classifications

    def extract_answers(self, organized_grid, image):
        """Extract answers from organized grid"""
        answers = {}

        for question_num, data in organized_grid.items():
            bubbles = data['bubbles']
            subject = data['subject']

            # Classify each bubble
            classifications = self.classify_bubbles(image, bubbles)

            # Determine answer
            filled_indices = [i for i, filled in enumerate(classifications) if filled]

            if len(filled_indices) == 1:
                # Single clear mark
                answer_options = ['a', 'b', 'c', 'd']
                answer = answer_options[filled_indices[0]]
                confidence = 'high'
                reason = 'single_clear_mark'
            elif len(filled_indices) > 1:
                # Multiple marks - take the darkest one
                answer_options = ['a', 'b', 'c', 'd']

                # Find darkest bubble among filled ones
                darkest_idx = 0
                darkest_intensity = 255

                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                for idx in filled_indices:
                    x, y, r = bubbles[idx]
                    mask = np.zeros(gray.shape, dtype=np.uint8)
                    cv2.circle(mask, (x, y), max(1, r - 2), 255, -1)
                    bubble_pixels = gray[mask == 255]

                    if len(bubble_pixels) > 0:
                        intensity = np.mean(bubble_pixels)
                        if intensity < darkest_intensity:
                            darkest_intensity = intensity
                            darkest_idx = idx

                answer = answer_options[darkest_idx]
                confidence = 'low'
                reason = 'multiple_marks_detected'
            else:
                # No marks
                answer = None
                confidence = 'none'
                reason = 'no_mark_detected'

            answers[question_num] = {
                'subject': subject,
                'answer': answer,
                'confidence': confidence,
                'reason': reason,
                'bubble_count': len(bubbles),
                'filled_count': len(filled_indices)
            }

        return answers

    def process_image(self, image):
        """Main processing function"""
        try:
            # Detect circles
            circles = self.detect_circles(image)

            if not circles:
                return {'error': 'No bubbles detected in image. Please ensure clear bubble markings.'}

            # Organize into grid
            organized_grid = self.organize_bubbles_into_grid(circles, image.shape)

            if not organized_grid:
                return {'error': 'Could not organize bubbles into grid structure. Check image alignment.'}

            # Extract answers
            answers = self.extract_answers(organized_grid, image)

            return {
                'extracted_answers': answers,
                'grid_info': {
                    'total_circles': len(circles),
                    'organized_questions': len(organized_grid),
                    'subjects_detected': list(set(data['subject'] for data in organized_grid.values()))
                },
                'metadata': {
                    'image_dimensions': list(image.shape),
                    'total_questions': len(answers),
                    'processing_status': 'completed'
                }
            }

        except Exception as e:
            return {'error': f'Processing failed: {str(e)}'}

# Initialize session state
if 'omr_processor' not in st.session_state:
    st.session_state.omr_processor = OMRProcessor()

if 'answer_key_loader' not in st.session_state:
    st.session_state.answer_key_loader = AnswerKeyLoader()

if 'scoring_engine' not in st.session_state:
    st.session_state.scoring_engine = ScoringEngine(st.session_state.answer_key_loader)

if 'processed_results' not in st.session_state:
    st.session_state.processed_results = []

def main():
    st.markdown('<h1 class="main-header">üìù OMR Evaluation & Scoring System</h1>', unsafe_allow_html=True)

    # Sidebar
    st.sidebar.title("üìã Navigation")
    page = st.sidebar.selectbox("Choose a page:", [
        "üéØ Process & Score OMR",
        "üìä View Results & Statistics",
        "üì§ Batch Processing",
        "üìã Answer Key Manager",
        "‚ÑπÔ∏è About"
    ])

    # Show answer key status
    if st.session_state.answer_key_loader.answer_keys:
        sets_loaded = list(st.session_state.answer_key_loader.answer_keys.keys())
        st.sidebar.success(f"‚úÖ Answer keys loaded: Set {', Set '.join(sets_loaded)}")
    else:
        st.sidebar.warning("‚ö†Ô∏è No answer keys loaded")

    if page == "üéØ Process & Score OMR":
        process_and_score_page()
    elif page == "üìä View Results & Statistics":
        results_statistics_page()
    elif page == "üì§ Batch Processing":
        batch_processing_page()
    elif page == "üìã Answer Key Manager":
        answer_key_manager_page()
    elif page == "‚ÑπÔ∏è About":
        about_page()

def answer_key_manager_page():
    st.header("üìã Answer Key Manager")

    st.info("""
    üìã **Upload Answer Key Excel File:**

    **Supported Formats:**
    1. **New Format (Recommended)**: Single sheet with subject columns (Python, EDA, SQL, Power BI, Statistics)
       - Each cell format: "question_number - answer_option" (e.g., "1 - a", "21 - b")
       - Supports multi-option answers: "16 - a,b,c,d"
       - System automatically creates both Set A and Set B from same data

    2. **Legacy Format**: Separate sheets named 'Set - A', 'Set - B', etc.
       - Each sheet should have answer columns for subjects
       - Direct answers: A, B, C, or D
    """)

    uploaded_excel = st.file_uploader(
        "Upload Answer Key Excel File",
        type=['xlsx', 'xls'],
        help="Upload Excel file containing answer keys"
    )

    if uploaded_excel is not None:
        if st.button("üì• Load Answer Keys", type="primary"):
            with st.spinner("Loading answer keys..."):
                success = st.session_state.answer_key_loader.load_from_excel(uploaded_excel)

                if success:
                    st.success("‚úÖ Answer keys loaded successfully!")

                    # Show loaded sets
                    for set_letter, answers in st.session_state.answer_key_loader.answer_keys.items():
                        with st.expander(f"üìö Set {set_letter} ({len(answers)} questions)"):
                            # Group by subject
                            subjects = {}
                            for q_num, data in answers.items():
                                subject = data['subject']
                                if subject not in subjects:
                                    subjects[subject] = []
                                subjects[subject].append((q_num, data['answer']))

                            for subject, questions in subjects.items():
                                st.write(f"**{subject}:**")
                                questions.sort()
                                answers_str = ', '.join([f"Q{q}: {a}" for q, a in questions[:5]])
                                if len(questions) > 5:
                                    answers_str += f" ... (showing first 5 of {len(questions)})"
                                st.write(answers_str)
                else:
                    st.error("‚ùå Failed to load answer keys. Check file format.")

def process_and_score_page():
    st.header("üéØ Process & Score OMR Sheet")

    if not st.session_state.answer_key_loader.answer_keys:
        st.warning("‚ö†Ô∏è Please upload answer keys first using the 'Answer Key Manager' page.")
        return

    # Student Information Input
    st.subheader("üë§ Student Information")
    col1, col2, col3 = st.columns(3)

    with col1:
        student_name = st.text_input("Student Name", placeholder="Enter student name")
    with col2:
        student_id = st.text_input("Student ID", placeholder="Enter student ID")
    with col3:
        set_letter = st.selectbox("Answer Set", list(st.session_state.answer_key_loader.answer_keys.keys()))

    # Instructions
    st.info("""
    üìã **Instructions:**
    - Upload a clear, well-lit OMR sheet image
    - Ensure bubbles are circular and completely filled
    - Select the correct answer set (A, B, etc.)
    - Enter student information for proper record keeping
    """)

    # File upload
    uploaded_file = st.file_uploader(
        "Upload OMR Sheet Image",
        type=['jpg', 'jpeg', 'png', 'bmp', 'tiff'],
        help="Upload a clear image of the OMR sheet"
    )

    if uploaded_file is not None:
        # Display uploaded image
        col1, col2 = st.columns([1, 1])

        with col1:
            st.subheader("üìÑ Uploaded Image")
            image = Image.open(uploaded_file)
            st.image(image, use_column_width=True)

            # Image info
            st.info(f"**Image Info:**\n"
                   f"- Size: {image.size[0]} x {image.size[1]} pixels\n"
                   f"- Mode: {image.mode}\n"
                   f"- Student: {student_name or 'Not specified'}\n"
                   f"- Set: {set_letter}")

        # Process button
        if st.button("üöÄ Process & Score OMR Sheet", type="primary"):
            with col2:
                with st.spinner("Processing OMR sheet and calculating scores..."):
                    try:
                        # Convert PIL image to OpenCV format
                        image_array = np.array(image)
                        if len(image_array.shape) == 3:
                            image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)

                        # Process the image
                        processing_results = st.session_state.omr_processor.process_image(image_array)

                        if 'error' in processing_results:
                            st.error(f"‚ùå Processing Error: {processing_results['error']}")
                            return

                        # Score the results
                        student_info = {
                            'name': student_name,
                            'id': student_id,
                            'image_name': uploaded_file.name
                        }

                        scoring_results = st.session_state.scoring_engine.score_omr_sheet(
                            processing_results['extracted_answers'],
                            set_letter,
                            student_info
                        )

                        if 'error' in scoring_results:
                            st.error(f"‚ùå Scoring Error: {scoring_results['error']}")
                            return

                        # Store results
                        combined_results = {
                            'processing': processing_results,
                            'scoring': scoring_results,
                            'timestamp': datetime.now().isoformat()
                        }

                        st.session_state.processed_results.append(combined_results)

                        # Display results
                        display_scored_results(scoring_results, processing_results)

                    except Exception as e:
                        st.error(f"Error processing image: {str(e)}")

def display_scored_results(scoring_results, processing_results):
    """Display scored OMR results"""
    st.subheader("üéØ Scoring Results")

    # Student Info
    student_info = scoring_results['student_info']
    if student_info:
        st.markdown(f"**Student:** {student_info.get('name', 'N/A')} | **ID:** {student_info.get('id', 'N/A')} | **Set:** {scoring_results['set']}")

    # Overall Score
    total_score = scoring_results['total_score']
    percentage = scoring_results['percentage']

    # Color code based on performance
    if percentage >= 80:
        score_class = "score-high"
        grade = "A"
    elif percentage >= 60:
        score_class = "score-medium"
        grade = "B"
    else:
        score_class = "score-low"
        grade = "C"

    st.markdown(f"""
    <div class="{score_class}">
        üéØ <strong>Total Score: {total_score}/100 ({percentage:.1f}%) - Grade: {grade}</strong>
    </div>
    """, unsafe_allow_html=True)

    # Summary metrics
    summary = scoring_results['summary']
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("‚úÖ Correct", summary['correct'])
    with col2:
        st.metric("‚ùå Incorrect", summary['incorrect'])
    with col3:
        st.metric("‚ö™ Unanswered", summary['unanswered'])
    with col4:
        st.metric("üìä Accuracy", f"{(summary['correct']/summary['total_questions']*100):.1f}%")

    # Subject-wise performance
    st.subheader("üìö Subject-wise Performance")

    subject_data = []
    for subject, scores in scoring_results['subject_scores'].items():
        subject_data.append({
            'Subject': subject,
            'Correct': scores['correct'],
            'Total': scores['total'],
            'Score': f"{scores['score']}/20",
            'Percentage': f"{scores['percentage']:.1f}%"
        })

    df_subjects = pd.DataFrame(subject_data)
    st.dataframe(df_subjects, use_container_width=True)

    # Performance chart
    fig = px.bar(
        df_subjects,
        x='Subject',
        y='Correct',
        title='Subject-wise Correct Answers',
        color='Correct',
        color_continuous_scale='RdYlGn'
    )
    fig.update_layout(showlegend=False)
    st.plotly_chart(fig, use_container_width=True)

    # Detailed question analysis
    st.subheader("üìù Detailed Question Analysis")

    # Group by subject
    subjects = {}
    for q_num, details in scoring_results['question_details'].items():
        subject = details['subject']
        if subject not in subjects:
            subjects[subject] = []
        subjects[subject].append((int(q_num), details))

    # Display by subject
    for subject, questions in subjects.items():
        with st.expander(f"üìö {subject} - Detailed Analysis", expanded=False):
            questions.sort(key=lambda x: x[0])

            question_data = []
            for q_num, details in questions:
                status_icon = "‚úÖ" if details['status'] == 'correct' else "‚ùå" if details['status'] == 'incorrect' else "‚ö™"

                question_data.append({
                    'Question': q_num,
                    'Status': f"{status_icon} {details['status'].title()}",
                    'Correct Answer': details['correct_answer'],
                    'Student Answer': details['student_answer'] or 'None',
                    'Confidence': details['confidence'],
                    'Score': f"{details['score']}/{details['marks']}"
                })

            df_questions = pd.DataFrame(question_data)
            st.dataframe(df_questions, use_container_width=True)

    # Debug Section - Show Raw Extracted Data
    st.subheader("üîç Debug Information")

    with st.expander("üìÑ Raw Extracted Answers (JSON)", expanded=False):
        st.json(processing_results['extracted_answers'])

    with st.expander("üîß Processing Metadata", expanded=False):
        st.json(processing_results['metadata'])
        st.json(processing_results['grid_info'])

    # Download results
    results_json = json.dumps({
        'scoring': scoring_results,
        'processing': processing_results
    }, indent=2)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"omr_scored_results_{timestamp}.json"

    st.download_button(
        label="üì• Download Detailed Results (JSON)",
        data=results_json,
        file_name=filename,
        mime="application/json"
    )

def results_statistics_page():
    st.header("üìä Results & Statistics")

    if not st.session_state.processed_results:
        st.info("üìù No results available. Process some OMR sheets first.")
        return

    # Overall statistics
    st.subheader("üìà Overall Statistics")

    total_sheets = len(st.session_state.processed_results)
    total_students = len([r for r in st.session_state.processed_results if r['scoring']['student_info'].get('name')])

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Sheets Processed", total_sheets)
    with col2:
        st.metric("Students with Names", total_students)
    with col3:
        avg_score = np.mean([r['scoring']['percentage'] for r in st.session_state.processed_results])
        st.metric("Average Score", f"{avg_score:.1f}%")

    # Results table
    st.subheader("üìã All Results")

    results_data = []
    for i, result in enumerate(st.session_state.processed_results):
        scoring = result['scoring']
        student_info = scoring['student_info']

        results_data.append({
            'Sl No.': i + 1,
            'Student Name': student_info.get('name', 'N/A'),
            'Student ID': student_info.get('id', 'N/A'),
            'Set': scoring['set'],
            'Score': f"{scoring['total_score']}/100",
            'Percentage': f"{scoring['percentage']:.1f}%",
            'Correct': scoring['summary']['correct'],
            'Incorrect': scoring['summary']['incorrect'],
            'Unanswered': scoring['summary']['unanswered'],
            'Timestamp': scoring['timestamp']
        })

    df_results = pd.DataFrame(results_data)
    st.dataframe(df_results, use_container_width=True)

    # Export to Excel
    if st.button("üìä Export Results to Excel"):
        # Create detailed Excel export
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            # Summary sheet
            df_results.to_excel(writer, sheet_name='Summary', index=False)

            # Detailed results for each student
            for i, result in enumerate(st.session_state.processed_results):
                scoring = result['scoring']
                student_name = scoring['student_info'].get('name', f'Student_{i+1}')

                # Create detailed sheet for this student
                detailed_data = []
                for q_num, details in scoring['question_details'].items():
                    detailed_data.append({
                        'Question': q_num,
                        'Subject': details['subject'],
                        'Correct Answer': details['correct_answer'],
                        'Student Answer': details['student_answer'] or 'None',
                        'Status': details['status'],
                        'Score': details['score'],
                        'Confidence': details['confidence']
                    })

                df_detailed = pd.DataFrame(detailed_data)
                sheet_name = f"{student_name[:20]}_{i+1}"  # Excel sheet name limit
                df_detailed.to_excel(writer, sheet_name=sheet_name, index=False)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        st.download_button(
            label="üì• Download Excel Report",
            data=output.getvalue(),
            file_name=f"omr_results_report_{timestamp}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    # Score distribution chart
    if st.session_state.processed_results:
        st.subheader("üìä Score Distribution")
        scores = [r['scoring']['percentage'] for r in st.session_state.processed_results]

        fig = px.histogram(
            x=scores,
            nbins=20,
            title='Score Distribution',
            labels={'x': 'Percentage Score', 'y': 'Number of Students'}
        )
        st.plotly_chart(fig, use_container_width=True)

def batch_processing_page():
    st.header("üì§ Batch Processing")

    if not st.session_state.answer_key_loader.answer_keys:
        st.warning("‚ö†Ô∏è Please upload answer keys first using the 'Answer Key Manager' page.")
        return

    st.info("Upload multiple OMR sheets for batch processing with scoring")

    # Batch settings
    st.subheader("‚öôÔ∏è Batch Settings")
    col1, col2 = st.columns(2)

    with col1:
        batch_set_letter = st.selectbox("Answer Set for All Images",
                                       list(st.session_state.answer_key_loader.answer_keys.keys()))
    with col2:
        auto_name = st.checkbox("Auto-generate student names (Student_001, Student_002, etc.)")

    uploaded_files = st.file_uploader(
        "Upload Multiple OMR Sheets",
        type=['jpg', 'jpeg', 'png', 'bmp', 'tiff'],
        accept_multiple_files=True,
        help="Upload multiple OMR sheet images"
    )

    if uploaded_files and st.button("üöÄ Process All Sheets with Scoring", type="primary"):
        batch_results = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, file in enumerate(uploaded_files):
            status_text.text(f"Processing: {file.name} ({i+1}/{len(uploaded_files)})")

            try:
                # Process image
                image = Image.open(file)
                image_array = np.array(image)
                if len(image_array.shape) == 3:
                    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)

                processing_results = st.session_state.omr_processor.process_image(image_array)

                if 'error' not in processing_results:
                    # Score results
                    student_info = {
                        'name': f"Student_{i+1:03d}" if auto_name else f"Student_from_{file.name}",
                        'id': f"ID_{i+1:03d}",
                        'image_name': file.name
                    }

                    scoring_results = st.session_state.scoring_engine.score_omr_sheet(
                        processing_results['extracted_answers'],
                        batch_set_letter,
                        student_info
                    )

                    if 'error' not in scoring_results:
                        # Store results
                        combined_results = {
                            'processing': processing_results,
                            'scoring': scoring_results,
                            'timestamp': datetime.now().isoformat()
                        }

                        st.session_state.processed_results.append(combined_results)

                        batch_results.append({
                            'Filename': file.name,
                            'Student': student_info['name'],
                            'Score': f"{scoring_results['total_score']}/100",
                            'Percentage': f"{scoring_results['percentage']:.1f}%",
                            'Correct': scoring_results['summary']['correct'],
                            'Status': 'Success'
                        })
                    else:
                        batch_results.append({
                            'Filename': file.name,
                            'Student': 'N/A',
                            'Score': 'N/A',
                            'Percentage': 'N/A',
                            'Correct': 0,
                            'Status': f'Scoring Error: {scoring_results["error"]}'
                        })
                else:
                    batch_results.append({
                        'Filename': file.name,
                        'Student': 'N/A',
                        'Score': 'N/A',
                        'Percentage': 'N/A',
                        'Correct': 0,
                        'Status': f'Processing Error: {processing_results["error"]}'
                    })

            except Exception as e:
                batch_results.append({
                    'Filename': file.name,
                    'Student': 'N/A',
                    'Score': 'N/A',
                    'Percentage': 'N/A',
                    'Correct': 0,
                    'Status': f'Error: {str(e)}'
                })

            progress_bar.progress((i + 1) / len(uploaded_files))

        status_text.text("Batch processing complete!")

        # Display batch results
        if batch_results:
            st.subheader("üìä Batch Processing Results")
            df_batch = pd.DataFrame(batch_results)
            st.dataframe(df_batch, use_container_width=True)

            # Summary metrics
            successful = df_batch[df_batch['Status'] == 'Success']
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("Total Files", len(df_batch))
            with col2:
                st.metric("Successful", len(successful))
            with col3:
                st.metric("Success Rate", f"{(len(successful)/len(df_batch)*100):.1f}%")
            with col4:
                if len(successful) > 0:
                    avg_score = pd.to_numeric(successful['Percentage'].str.replace('%', ''), errors='coerce').mean()
                    st.metric("Average Score", f"{avg_score:.1f}%")

def about_page():
    st.header("‚ÑπÔ∏è About OMR Evaluation & Scoring System")

    st.markdown("""
    ## üéØ Complete OMR Solution
    This comprehensive OMR (Optical Mark Recognition) system provides end-to-end functionality for
    processing bubble sheet answer sheets and automatically scoring them against answer keys.

    ## üöÄ Key Features

    ### üìù OMR Processing:
    - **Multi-Parameter Detection**: HoughCircles with 3 different parameter sets
    - **Column-Based Organization**: K-means clustering for spatial grid organization
    - **Statistical Classification**: Advanced bubble analysis using intensity statistics
    - **Confidence Scoring**: High/Low/None with detailed reasoning

    ### üéØ Answer Key & Scoring:
    - **Excel Answer Key Support**: Load answer keys from Excel files (Set A, Set B, etc.)
    - **Automatic Scoring**: Compare student answers against correct answers
    - **Subject-wise Analysis**: Detailed performance breakdown by subject
    - **Grade Calculation**: Automatic percentage and grade assignment

    ### üìä Results Management:
    - **Student Information**: Track student names, IDs, and timestamps
    - **Detailed Analytics**: Question-by-question analysis with confidence levels
    - **Batch Processing**: Process multiple sheets simultaneously
    - **Excel Export**: Comprehensive reporting in Excel format
    - **Statistics Dashboard**: Overall performance metrics and charts

    ## üõ†Ô∏è Technical Implementation
    - **OpenCV**: Advanced image processing and circle detection
    - **Scikit-learn**: Machine learning for spatial clustering
    - **Pandas**: Data manipulation and Excel file handling
    - **Plotly**: Interactive visualizations and charts
    - **Streamlit**: Professional web interface

    ## üìä Expected Performance
    - **Processing Speed**: 3-8 seconds per sheet
    - **Grid Format**: 5 columns √ó 20 rows √ó 4 options (A,B,C,D)
    - **Subjects Supported**: PYTHON, EDA, SQL, POWER BI, ADV STATS
    - **Accuracy**: 85-95% for high-quality images

    ## üéì Usage Workflow

    ### 1. **Setup Answer Keys** üìã
    - Upload Excel file with answer keys
    - Support for multiple sets (A, B, etc.)
    - Automatic validation and loading

    ### 2. **Process OMR Sheets** üîç
    - Upload individual or multiple OMR images
    - Enter student information
    - Select appropriate answer set
    - Automatic processing and scoring

    ### 3. **View Results** üìä
    - Detailed student performance analysis
    - Subject-wise breakdowns
    - Confidence levels and error analysis
    - Statistical summaries

    ### 4. **Export Reports** üì•
    - Excel format with multiple sheets
    - JSON format for integration
    - Comprehensive student records

    ## üìã Excel Answer Key Format

    Your Excel file should contain:
    - **Sheet Names**: 'Set - A', 'Set - B', etc.
    - **Columns**: Subject-wise answer columns
    - **Answers**: A, B, C, or D for each question
    - **Structure**: 20 questions per subject

    ---

    **Perfect for educational institutions, training centers, and assessment organizations** üéìüìö
    """)

if __name__ == "__main__":
    main()